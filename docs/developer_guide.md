# DEVELOP
You will need the following dependencies installed

* make
* python39
* python-39-devel (so gssapi wheel can be built)
* krb5-workstation (provides access to services using kerberos auth ex. OSIDB)
* virtualenv (if you do not want to install python deps into your system)

Install and activate a Python virtual environment:
```bash
> python3.9 -m venv venv  # Create Python virtual environment
> source venv/bin/activate  # Enable virtual env
```
Install pip-tools and development requirements:
```bash
> pip install pip-tools  # Install pip-tools
> pip-sync requirements/dev.txt
```

Alternatively, replace the pip-sync call with:
```bash
> make install-dev-deps
```

To install from source
```bash
> pip install .
```

## Development

Raising issues

### Running tests

### Using pip-tools
Griffon has adopted `pip-tools` as its tool of choice for python dependency management,
in this section we'll go over the basics, the similarities and the differences between `pip-tools` and `pip`,
as well as how to use it effectively.

With `pip`, adding a dependency is as simple as adding it to the `requirements.txt`,
and optionally choosing which version(s) to use.

With `pip-tools`, the dependency (versioned or not) is added to either `requirements.in`,
`devel-requirements.in` or `local-requirements.in`, then we must execute the `pip-compile`
command in order to generate the corresponding `*-requirements.txt`.

```bash
$ pip-compile --generate-hashes --allow-unsafe	# this will compile requirements.in -> requirements.txt
$ pip-compile --generate-hashes --allow-unsafe devel-requirements.in	# be explicit for alternate requirements files
```

Instead of typing these commands manually you can simply do

```bash
$ make compile-deps
```

and all the necessary `requirements.txt` files will be compiled correctly.

So far the differences between the two are minimal, both use a `requirements.txt` file to express its dependencies, however the dependency tree generated by `pip-compile` is more thorough, it will include all implicit dependencies of the ones explicitly defined in the `*.in` files and will pin them to a very specific version.
Not only does this make it easier to reproduce prod/dev environments, but it can also be helpful for later security vulnerabilities scanning.

Note that if any dependencies are added to the `*.in` files, and then `pip-compile` is ran, the versions of the existing pinned dependencies will not change and only the new dependencies will be added to the `requirements.txt`

Updating dependencies with `pip` and `pip-tools` is largely the same, the command for doing so with `pip-tools` is the following

```bash
$ pip-compile --generate-hashes --allow-unsafe --upgrade-package django --upgrade-package requests==2.0.0
```

To install the dependencies with `pip`, you simply pass the requirements file(s) to the `-r` option and all the requirements in the file will be installed, even if the file was generated by `pip-compile`!

With `pip-tools`, the command for installing dependencies is `pip-sync requirements.txt` (or any other file generated by `pip-compile`), however `pip-sync` will not only install the requirements, but it will also uninstall any packages or versions that do **not** match the one defined in the requirements file.

To sync up with latest generated requirements/*.txt
```bash
$ make sync-deps
```

> :warning: Make sure to run `pip-sync` within a virtual environment, otherwise you risk having system-wide packages that are not in the `requirements.txt` be uninstalled

As for what each requirements file holds, here's a quick explanation for each:
- `requirements/dev.txt`: dependencies necessary for developing
- `requirements/base.txt`: 
- `requirements/test.txt`: 

For more information on `pip-tools` and its usage, check the [official documentation](https://pip-tools.readthedocs.io/en/latest/).


